{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import applications as kapp\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras import layers as kl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import imageio\n",
    "from tensorflow_docs.vis import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "INPUT_SIZE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "SEQ_LENGTH = 20\n",
    "DATASET_DIR = r'data\\UCF-101'\n",
    "NUM_FEATURES = 2048\n",
    "EPOCHS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "\n",
    "def crop_center_square(frame):\n",
    "  y, x = frame.shape[0:2]\n",
    "  min_dim = min(y, x)\n",
    "  start_x = (x // 2) - (min_dim // 2)\n",
    "  start_y = (y // 2) - (min_dim // 2)\n",
    "  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
    "\n",
    "def to_gif(images):\n",
    "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
    "  imageio.mimsave('./animation.gif', converted_images, fps=25)\n",
    "  return embed.embed_file('./animation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frames extraction\n",
    "\n",
    "def frames_extraction(path, resize=(224, 224)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    video_lenght = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = max(int(video_lenght/SEQ_LENGTH), 1)\n",
    "\n",
    "    try:\n",
    "        for i in range(SEQ_LENGTH):\n",
    "            cap.set(cv2.CAP_PROP_FRAME_COUNT, i*interval)\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            normalized_frame = frame / 255.0\n",
    "            frames.append(normalized_frame)\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    input = tf.keras.Input(INPUT_SIZE)\n",
    "    cnn = kapp.ResNet50(pooling=\"avg\", include_top = False, weights='imagenet', input_shape = INPUT_SIZE)\n",
    "    \n",
    "    preprocessed = preprocess_input(input)\n",
    "\n",
    "    output = cnn(preprocessed)\n",
    "    cnn_model = tf.keras.Model(input, output, name=\"ResNet50\")\n",
    "\n",
    "    cnn_model.summary()\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = create_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model(cnn_model, to_file=\"resnet50_structure_plot.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "label = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "label_vocab = label.get_vocabulary()\n",
    "print(label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df, dir):\n",
    "    samples = len(df)\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = (label(labels[..., None])).numpy()\n",
    "    \n",
    "    frame_masks = np.zeros(shape=(samples, SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(samples, SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    indx = 0\n",
    "\n",
    "    for class_name in (label_vocab):\n",
    "        video_names = df.loc[df[\"tag\"] == class_name][\"video_name\"]\n",
    "        for video_idx, video_name in enumerate(video_names):\n",
    "            video_path = os.path.join(dir, class_name, video_name)\n",
    "            frames = frames_extraction(video_path)\n",
    "            frames = frames[None, ...]\n",
    "\n",
    "            temp_frame_mask = np.zeros(shape=(1, SEQ_LENGTH,), dtype=\"bool\",)\n",
    "            temp_frame_features = np.zeros(shape=(1, SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "            for i, batch in enumerate(frames):\n",
    "                video_length = batch.shape[0]\n",
    "                length = min(SEQ_LENGTH, video_length)\n",
    "                for j in range(length):\n",
    "                    temp_frame_features[i, j, :] = cnn_model.predict(batch[None, j, :], verbose=0,)\n",
    "                temp_frame_mask[i, :length] = 1\n",
    "\n",
    "            frame_features[indx+video_idx,] = temp_frame_features.squeeze()\n",
    "            frame_masks[indx+video_idx,] = temp_frame_mask.squeeze()\n",
    "            \n",
    "        indx += len(video_names)\n",
    "\n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = feature_extraction(train_df, DATASET_DIR)\n",
    "test_data, test_labels = feature_extraction(test_df, DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "\n",
    "def create_model():\n",
    "    input_mask = keras.Input((SEQ_LENGTH, ), dtype=\"bool\")\n",
    "    input_features = keras.Input((SEQ_LENGTH, NUM_FEATURES))\n",
    "    inputs = [input_features, input_mask]\n",
    "\n",
    "    x = kl.LSTM(64, return_sequences=True)(input_features, mask=input_mask)\n",
    "    x = kl.LSTM(32, return_sequences=True)(x)\n",
    "    x = kl.LSTM(16)(x)\n",
    "    x = kl.Dropout(0.4)(x)\n",
    "    output = kl.Dense(len(label_vocab), activation=\"softmax\")(x)\n",
    "    model = keras.Model([inputs], output, name=\"LSTM_Model\")\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model(model, to_file=\"lstm_model_structure_plot.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/tmp/video_classifier/ckpt.weights.h5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, save_weights_only=True, save_best_only=True, verbose=1)\n",
    "\n",
    "early_stooping_callback = keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=10, mode=\"min\", restore_best_weights=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_history = model.fit(x=[train_data[0], train_data[1]], y=train_labels, epochs=EPOCHS, validation_split=0.3, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "_, model_evaluation = model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "print(f\"Accuracy: {round(model_evaluation * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss & accuracy\n",
    "\n",
    "def plot_compare(model_history, metric_1, metric_2, title):\n",
    "    value_1 = model_history.history[metric_1]\n",
    "    value_2 = model_history.history[metric_2]\n",
    "    x = range(len(value_1))\n",
    "\n",
    "    plt.plot(x, value_1, label=metric_1)\n",
    "    plt.plot(x, value_2, label=metric_2)\n",
    "    plt.grid(True)\n",
    "    plt.title(title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare(model_training_history, \"loss\", \"val_loss\", \"Total Loss vs Total Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare(model_training_history, \"accuracy\", \"val_accuracy\", \"Total Accuracy vs Total Validation Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
